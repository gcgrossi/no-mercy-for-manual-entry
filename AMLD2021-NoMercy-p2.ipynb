{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"AMLD2021-NoMercy-p2.ipynb","provenance":[],"collapsed_sections":["ylv3VBMHA_hu","qB8AE71kZcJs","H5Ug120P0MDH","8rEBiccQ4oIf","Hk3-53-9Xj83","mQjDeb5LHl0K","72rkDz-FHhD3","nz3wes-2HhD3","SoKI7Wd5HhD4","Lqo3UUbHGPFh"],"authorship_tag":"ABX9TyM/xZwztVTEuBg3D8DrdoFL"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8lgAaZKKALCy"},"source":["<img src=\"https://gcgrossi.github.io/GiulioGrossi.png\" width=\"20%\">\n","\n","# **_Giulio Cornelio Grossi, Ph.D._** \n","_giulio.cornelio.grossi@gmail.com_\n","\n","[![Linkedin](https://img.shields.io/badge/Linkedin-blue?style=for-the-badge&logo=linkedin)](https://www.linkedin.com/in/giulio-cornelio-grossi/)\n","[![Github](https://img.shields.io/badge/Github-black?style=for-the-badge&logo=github)](https://github.com/gcgrossi)\n","\n","[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/118ZV814gCWS1PmJwXEQG0IoLoy0-Dcso?usp=sharing)\n"]},{"cell_type":"markdown","metadata":{"id":"ylv3VBMHA_hu"},"source":["#**_Dowloads and Installs_**"]},{"cell_type":"code","metadata":{"id":"3rMtNXYe5i2f"},"source":["print('\\n Installing Tesseract ... \\n')\n","\n","# intstall pytesseract \n","!pip install pytesseract\n","!apt install tesseract-ocr\n","!apt install libtesseract-dev\n","\n","print('\\n Cloning Github Repository ... \\n')\n","\n","# clone repository with datatset\n","!rm -r AMLD2021/\n","!git clone https://github.com/SamurAi-sarl/AMLD2021.git\n","!ls -ltr AMLD2021/*\n","\n","print('\\n Downloading example images ... \\n')\n","\n","#download relevant images\n","!curl -L \"https://docs.google.com/uc?export=download&id=1Gu46DRx_idNbvjqHcuSKilX64v0DYNUJ\" > transaction_ticket.jpg\n","!curl -L \"https://docs.google.com/uc?export=download&id=1fKDIVs2JcGxe3i01RtPpDKIbKx1Oan7Z\" > registration_ticket.jpg\n","!curl -L \"https://docs.google.com/uc?export=download&id=19rjKuqF5s9AfAQiUt80RbLkQVP3oQZy9\" > invoice_ticket.jpg\n","!curl -L \"https://docs.google.com/uc?export=download&id=1UV4wnGKG3S5YU9PGesXcDJWrMLZIsK7z\" > invoice_scanned.jpg\n","!curl -L \"https://docs.google.com/uc?export=download&id=1i-HOc4VE0U2J-p1so5mhHAnc1e9rdaFR\" > invoice_test_flash.jpg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qB8AE71kZcJs"},"source":["#**_Imports_**"]},{"cell_type":"code","metadata":{"id":"0JIWi4hbZfiX"},"source":["from google.colab import drive\n","from google.colab import files\n","\n","from pathlib import Path\n","\n","import pytesseract\n","pytesseract.pytesseract.tesseract_cmd = (r'/usr/bin/tesseract')\n","from pytesseract import Output\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import subprocess\n","import shutil\n","import json\n","import cv2\n","import sys\n","import os\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kPKVHVBsOOX3"},"source":["# mount drive folder and import custom modules\n","#drive.mount('/content/drive', force_remount=False)\n","#sys.path.insert(0,'/content/drive/MyDrive/Samurai_Workshop')\n","\n","#from architectures.smallvggnet import SmallVGGNet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H5Ug120P0MDH"},"source":["#**_Utilities Functions_** "]},{"cell_type":"markdown","metadata":{"id":"8rEBiccQ4oIf"},"source":["###**_Decode Bytestream Images_**"]},{"cell_type":"code","metadata":{"id":"OXxVD-QD4oNo"},"source":["def decode_image(vals):\n","  # decode uploaded bystring images\n","  nparr = np.fromstring(vals, np.uint8)\n","  return cv2.imdecode(nparr, cv2.IMREAD_COLOR)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hk3-53-9Xj83"},"source":["###**_Draw image in matplotlib_**"]},{"cell_type":"code","metadata":{"id":"T_kBnEtJXixW"},"source":["def draw(image,size=(7,25)):\n","  plt.figure(figsize=size)\n","  plt.imshow(image)\n","  return"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HpjYVouDaqSZ"},"source":["<img src=\"https://drive.google.com/uc?id=1q7PJ36Cx8-YHnxDrG9w-coxn-GL2QWwv\" width=\"20%\">\n","\n","#**_Difficulty: Padawan_**\n","#_Document with fixed position items_\n","\n","These kind of documents are the easiest to process. The methodology is very simple: since the position of the information is fixed, it is sufficient to select the corresponding boxes (in the form of x1,y1,x2,y2 coordinates) in the image and then perform a text extraction with Tesseract.\n"]},{"cell_type":"markdown","metadata":{"id":"kZcrNTv62en3"},"source":["##**_Example 1_**\n","##_Image: a Transaction Ticket_\n","\n","In this example, a real life transaction ticket from a famous bank has been delivered to the backoffice, that is now responsible to fill a Transactions Database with the trade information. Also the Asset Manager (or whoever is responsible for the task) should account for the transaction in his portfolio. We are going to read a couple of fields in the document just to give an example."]},{"cell_type":"code","metadata":{"id":"kPJc9k102lFB"},"source":["#read the image from disk\n","filename = os.path.join(os.getcwd(),'transaction_ticket.jpg')\n","template = cv2.imread(filename)\n","\n","# store a copy of the original image\n","orig = template.copy()\n","\n","#show the input image\n","draw(cv2.cvtColor(template, cv2.COLOR_BGR2RGB))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UYqdKB9O4n91"},"source":["### _**Select Relevant RoIs (Regions of Insterest)**_\n","\n"]},{"cell_type":"code","metadata":{"id":"htbHITfa7f8n"},"source":["# define the coordinates of different rois\n","x_start, x_end = 1165, 1585\n","y = {'type':(1183,1208),'direction':(1208,1234),'exchange':(1234,1259)}\n","\n","rois=[]\n","\n","# loop over the rois coordinates\n","# select the image region  \n","# and append a list with the selected rois\n","for key,val in y.items():\n","  y_start, y_end = val[0], val[1]\n","  roi = template[y_start:y_end,x_start:x_end]\n","  rois.append(roi)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OnxbiOjG-L1w"},"source":["### _**Extract the text from each RoI**_\n","\n"]},{"cell_type":"code","metadata":{"id":"ykH9hKAz-UZL"},"source":["#loop over the selected rois\n","# draw the roi and extract the text with tesseract\n","for roi in rois:\n","  rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n","  draw(rgb,(25,7))\n","  text = pytesseract.image_to_string(rgb)\n","\n","  for line in text.split(\"\\n\"):\n","      print(line)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EiE3YRwf_Z9o"},"source":["##**_Example 2_**\n","##_Image: a Registration Form and an Invoice_\n","In this second example, a patient registration form from a famous dental office in Geneva has been submitted. Our system is now taking charge of creating a folder for the patient to store all his relevant documents. Later on, the system receives an invoice that should be stored in the folder that matches the client to whom the invoice was addressed \n","\n","We are going read the first name and last name fields in the registration form (with the same methodology of our first example) and create a corresponding folder. After, we are going to extract the same information from the invoice, and try to move the invoice in the corresponding folder."]},{"cell_type":"code","metadata":{"id":"uu3xschl_Z9x"},"source":["#read the image from disk\n","filename = os.path.join(os.getcwd(),'registration_ticket.jpg')\n","template = cv2.imread(filename)\n","\n","# store a copy of the original image\n","orig = template.copy()\n","\n","#show the input image\n","draw(cv2.cvtColor(template, cv2.COLOR_BGR2RGB))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"km_Te8tJ_Z9x"},"source":["### _**Select Relevant RoIs (Regions of Insterest)**_\n","\n"]},{"cell_type":"code","metadata":{"id":"HwzG9-Y8_Z9x"},"source":["# define the coordinates of different rois\n","x_start, x_end = 200, 1400\n","y = {'first_name':(545,617),'last_name':(761,822)}\n","\n","rois=[]\n","\n","# loop over the rois coordinates\n","# select the image region  \n","# and append a list with the selected rois\n","for key,val in y.items():\n","  y_start, y_end = val[0], val[1]\n","  roi = template[y_start:y_end,x_start:x_end]\n","  rois.append(roi)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jxhqS043_Z9y"},"source":["### _**Extract the text from each RoI**_\n","\n"]},{"cell_type":"code","metadata":{"id":"7zeRQgHw_Z9y"},"source":["foldername=''\n","# define a list of forbidden tokens\n","forbidden_tokens = ['\\x0c']\n","\n","#loop over the selected rois\n","# draw the roi and extract the text with tesseract\n","print(\"\\n[INFO] Characters after splitting for endline token\")\n","for roi in rois:\n","  rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n","  draw(rgb,(25,7))\n","  text = pytesseract.image_to_string(rgb)\n","\n","  print('[INFO] {}'.format(text.split(\"\\n\")))\n","\n","  #remove the end line token and remove forbidden characters\n","  for line in text.split(\"\\n\"):\n","      if line and line not in forbidden_tokens: foldername+=line\n","\n","#creating the directory with os module\n","print('\\n[INFO] creating directory: {}\\n'.format(foldername))\n","os.mkdir(foldername)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ep9Ri300SqJu"},"source":["Here we see another important feature of Tesseract. As you can see from the output of the string processing, there are some unexpected characters detected `'\\x0C`' that, based on [this definition](https://www.computerhope.com/jargon/f/formfeed.htm), is: \n","\n","*a special character that, when encountered in code, causes printers to automatically advance one full page or the start of the next page.*\n","\n","This is a limitation of Tesseract that should be taken in cosideration when designing a Data Science product that should fit a specific case. One possible solution is to explore another method of Tessearct, `image_to_data` instead of `image_to_string`. "]},{"cell_type":"code","metadata":{"id":"0W2cr4oiVM2l"},"source":["df_text=[]\n","\n","# get data regarding the text as a dataframe \n","# append each dataframe to a list\n","for roi in rois:\n","  rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n","  df_text.append(pytesseract.image_to_data(rgb, output_type='data.frame')) #Output.DICT\n","\n","# show one output as example\n","df_text[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-TlPJJApZTjo"},"source":["The `image_to_data` method returns an handy data frame with more information (as the position of the box surrounding the word or the detection confidence). But most importantly there are less strange characters to deal with. From there is easy to extract again the information we need."]},{"cell_type":"code","metadata":{"id":"lfqzoH9VYUrn"},"source":["foldername=''\n","for df in df_text:\n","    for line in df['text'].dropna().to_list():\n","      if line: foldername+=line\n","\n","print('\\n[INFO] Extracted Folder Name: {}\\n'.format(foldername))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x7EVkDWFab_N"},"source":["### _**Read an Input Invoice**_\n","we will now repeat the steps for an input invoice."]},{"cell_type":"code","metadata":{"id":"T6tLN4SqaU_p"},"source":["#read the image from disk\n","filename = os.path.join(os.getcwd(),'invoice_ticket.jpg')\n","template = cv2.imread(filename)\n","\n","# store a copy of the original image\n","orig = template.copy()\n","\n","#show the input image\n","draw(cv2.cvtColor(template, cv2.COLOR_BGR2RGB))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3YHmwAPdoD23"},"source":["### _**Extract Text from RoI**_"]},{"cell_type":"code","metadata":{"id":"BGnD0eiMcp1V"},"source":["# define the coordinates of different rois\n","x_start, x_end = 130, 775\n","y_start, y_end = 440, 600\n","\n","# select the image region  \n","roi = template[y_start:y_end,x_start:x_end]\n","\n","# extract text as a dataframe\n","rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n","draw(rgb,(25,7))\n","df_text = pytesseract.image_to_data(rgb, output_type='data.frame') #pytesseract.image_to_string(rgb)\n","\n","df_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m72ZP7Y77P99"},"source":["# get a list of strings detected\n","# find elements between 'Name:' and 'Address:'\n","# construct the foldername string by joining the resulting list\n","text_list=df_text['text'].dropna().to_list()\n","idx_name = text_list.index('Name:')+1\n","idx_address = text_list.index('Address:')\n","foldername=''.join(text_list[idx_name:idx_address])\n","\n","print('\\n[INFO] Extracted Folder Name: {}\\n'.format(foldername))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hpaP-FK2mDgL"},"source":["### _**Move the invoice to another folder**_\n","\n","We can now move the invoice to the corresponding folder, adding one more feature: if we cannot find a match between the receiver of the invoice and our folders, we move it to an 'Unmatched' folder. We can also add more actions like sending a notification email or a message when there is no match. \n","\n","In this case, the human being enters in the loop by checking the unmatched invoiced and wether the algorithm has failed to detect a correct string. He reports to the error to the IT which will investigate further and improve the algorithm (if possible).\n","\n","In this way, a positive data loop closes itself, possibly leading to a better model as the time passes."]},{"cell_type":"code","metadata":{"id":"XAzj9vIelxF4"},"source":["# create an Unmatched directory if doesnt exist\n","if os.path.isdir(os.path.join(os.getcwd(),'Unmatched')):\n","  print('\\n[INFO] Unmatched directory already exists\\n')\n","else:\n","  os.mkdir('Unmatched')\n","\n","# find all the directories in the current folder and fill a list\n","a_dir = os.getcwd()\n","dirlist= [name for name in os.listdir(a_dir) if os.path.isdir(os.path.join(a_dir, name))]\n","\n","# if the foldername is in the list we move the invoice there\n","# otherwise we move it to the Unmatched folder\n","\n","destination = foldername if foldername in dirlist else 'Unmatched'\n","\n","try:\n","  shutil.copy(filename, os.path.join(os.getcwd(),destination))\n","  print(\"File copied successfully.\")\n","  \n","# If source and destination are same\n","except shutil.SameFileError:\n","  print(\"Source and destination represents the same file.\")\n"," \n","# If there is any permission issue\n","except PermissionError:\n","    print(\"Permission denied.\")\n"," \n","# For other errors\n","except:\n","    print(\"Error occurred while copying file.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x_AxAMFPqdre"},"source":["If you now browse to the directory (using the File icon in the right menu), you will see that the invoice has been copied to the correct folder. Well done!"]},{"cell_type":"markdown","metadata":{"id":"wWWlADPjo3Ei"},"source":["##**_Exercise_**\n","\n","<img src=\"https://drive.google.com/uc?id=10tpXK7FwcLv4NEc9zAyKNDynb-wCVYlb\" width=\"50%\">\n","\n","##_Extract Other Fields in the Invoice or the Transaction Ticket_\n","Use the invoice or the transaction ticket already dowloaded and the methods described above to extract other relevant fields."]},{"cell_type":"code","metadata":{"id":"XGwmvZGkpq3u"},"source":["# Use this cell to write your code"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2oU5FKAwSqkR"},"source":["<img src=\"https://drive.google.com/uc?id=1xFbyKxKkx-ljh8n8Y93NLrvG_Y-c_yz5\" width=\"25%\">\n","\n","#**_Difficulty: Novice_**\n","#_Scanned Document with fixed position items_\n","\n","#**_Exercise (Optional)_**\n","Please read and run the cells in this section to see what changes if we apply the same methology of RoI selection we did before to scanned documents."]},{"cell_type":"markdown","metadata":{"id":"PVV317ifSKTl"},"source":["We are going now to read a document that was already scanned using Cam Scanner, the famous Android App for document scanning. We repeat the steps applied during **Case 1** . Since the position of the items is still fixed, the methodology of defining RoIs is exactly the same. We will have some differences though, and we will analyse them."]},{"cell_type":"code","metadata":{"id":"1tDnC3xsSuym"},"source":["#read the image from disk\n","filename = os.path.join(os.getcwd(),'invoice_scanned.jpg')\n","template = cv2.imread(filename)\n","\n","#show the input image\n","draw(cv2.cvtColor(template, cv2.COLOR_BGR2RGB))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3N4Y_yZE3YUz"},"source":["**Very important**: the size of the input image may vary based on the different software/hardware used for scanning the document. In this case, the x,y coordinates of the original RoIs must absolutely be scaled (down or up) to match the image size, otherwise we will end up selecting a different region or we will even get an Error from OpenCV! \n","\n","Thus, we can calculte scaling factors for the height and the width of the input RoIs: \n","\n","$f_{w}= \\frac{w}{w_{original}} $\n","\n","$f_{h}= \\frac{h}{h_{original}} $\n","\n","where $w,h$ and $w_{original},h_{original}$ are the width and the height of the input image and the original image respectively"]},{"cell_type":"code","metadata":{"id":"gVeUWduUUkhL"},"source":["\n","# calculate the resizing factor for the rois\n","h_orig, w_orig, c_orig = orig.shape \n","h, w, c = template.shape \n","\n","fh = h/h_orig\n","fw = w/w_orig  \n","\n","# define the coordinates of different rois\n","x_start, x_end = int(130*fw), int(775*fw)\n","y_start, y_end = int(440*fh), int(600*fh)\n","\n","# select the image region  \n","roi = template[y_start:y_end,x_start:x_end]\n","\n","# extract text as a dataframe\n","rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n","draw(rgb,(25,7))\n","df_text = pytesseract.image_to_data(rgb, output_type='data.frame') #pytesseract.image_to_string(rgb)\n","\n","# get a list of strings detected\n","# find elements between 'Name:' and 'Address:'\n","# construct the foldername string by joining the resulting list\n","text_list=df_text['text'].dropna().to_list()\n","idx_name = text_list.index('Name:')+1\n","idx_address = text_list.index('Address:')\n","foldername=''.join(text_list[idx_name:idx_address])\n","\n","print('\\n[INFO] Extracted Folder Name: {}\\n'.format(foldername))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mb3OWBsB7D8d"},"source":["As you can see here the are two things to notice:\n","\n","\n","1.   The RoI now is not centered exactly\n","2.   The characters are way more blurred than the original image\n","\n","The blur does not represent a problem in this particular case, Tesseract still can find the information we were looking for. But unfortunately, not all the cases are equal, and there can be situations (documents with poorer quality) where the algorithm fails (as we will see soon).\n","\n","Also notice that, if we were looking for the email field (i.e. to send an email to the receiver), we would have failed. We would need to find a way to center the content of the RoI.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9T_o0Wba2Psv"},"source":["<img src=\"https://drive.google.com/uc?id=1sjHNgjLOC6d5aErNWPnO4BNPQn6SXSxQ\" width=\"25%\">\n","\n","#**Case 3 - _Difficulty: Master_**\n","#_Documents taken from camera: create an homemade scanner_\n","\n","Sometimes you will face the case when the documents are not just scanned, but are images taken from a camera. In this case, it will be impossible to apply the methodology with RoIs out of the box, because the document paper will never be in the correct position to extract a RoI. We need to process the image in order to align it to the original template. We need therefore to:\n"," \n","\n","1.   Detected the countour of the paper in the image\n","2.   project the contour to a new image with the correct alignment\n","\n","The procedure is a little bit heavy to work out, but it will be clear in the next section what are the logical passages to apply."]},{"cell_type":"markdown","metadata":{"id":"mQjDeb5LHl0K"},"source":["##**_Utilities Functions_**"]},{"cell_type":"markdown","metadata":{"id":"72rkDz-FHhD3"},"source":["###**_Four Point Transform_**\n","\n","Given an input image, a target image and a set of starting points, orders the input point from top-left to bottom-left in clockwise order. Calculates the perspective transform start $\\rightarrow$ target and applies it to the input image."]},{"cell_type":"code","metadata":{"id":"1UXZJwXIHhD3"},"source":["def order_points(pts):\n","  # initialzie a list of coordinates that will be ordered\n","  rect = np.zeros((4, 2), dtype = \"float32\")\n","  \n","  # the top-left point will have the smallest sum, whereas\n","  # the bottom-right point will have the largest sum\n","  s = pts.sum(axis = 1)\n","  rect[0] = pts[np.argmin(s)]\n","  rect[2] = pts[np.argmax(s)]\n","  \n","  # now, compute the difference between the points, the\n","  # top-right point will have the smallest difference,\n","  # whereas the bottom-left will have the largest difference\n","  diff = np.diff(pts, axis = 1)\n","  rect[1] = pts[np.argmin(diff)]\n","  rect[3] = pts[np.argmax(diff)]\n","  \n","  # return the ordered coordinates\n","  return rect\n","\n","def four_point_transform(image, target, pts):\n","  # obtain a consistent order of the points and unpack them\n","  # individually\n","  rect = order_points(pts)\n","  (tl, tr, br, bl) = rect\n","  \n","  # use the target image shape as destination point of the transformation\n","  h, w, c = target.shape \n","  dst = np.array([[0, 0],[w, 0],[w, h],[0, h]], dtype = \"float32\")\n","  \n","  # compute the perspective transform matrix and then apply it\n","  M = cv2.getPerspectiveTransform(rect, dst)\n","  warped = cv2.warpPerspective(image, M, (w, h))\n","  # return the warped image\n","  return warped"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nz3wes-2HhD3"},"source":["###**_Edge Detection_**\n","\n","**`auto_canny`** function: a slightly modified version of Canny that takes in input just one parameter ($\\sigma$). It calculates the median value of the pixel intensities ($\\mu$) and calls Canny algorithm with threshold boundaries $\\mu\\pm\\sigma$.\n","\n","**`edge_detection`** function: takes an input image, convert to grayscale, blur it and call the `auto_canny` function. Grabs the contours with max area found by Canny and approximate, them to the closest polygon. Returns a dictionary with the edge mask and the contour if the polygon has 4 edges. Returns False if nothing has been found."]},{"cell_type":"code","metadata":{"id":"Ck-Z45fzHhD3"},"source":["def auto_canny(image, sigma=0.33):\n","    # compute the median of the single channel pixel intensities\n","    v = np.median(image)\n","\n","    # apply automatic Canny edge detection using the computed median\n","    lower = int(max(0, (1.0 - sigma) * v))\n","    upper = int(min(255, (1.0 + sigma) * v))\n","    edged = cv2.Canny(image, lower, upper)\n","\n","    # return the edged image\n","    return edged\n","\n","\n","def edge_detection(image,sigma=0.33):\n","  # convert the image to grayscale, blur it, and find edges\n","  # in the image\n","  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","  gray = cv2.GaussianBlur(gray, (5, 5), 0)\n","  edged = auto_canny(gray,sigma)\n","  #edged = cv2.Canny(gray, thresh_low, thresh_high)\n","\n","  # find the contours with max area\n","  cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n","  cnts = sorted(cnts[0], key = cv2.contourArea, reverse = True)[:5]\n","\n","  # init output contour\n","  screenCnt = None\n","\n","  # loop over the contours\n","  for c in cnts:\n","    # approximate the contour\n","    peri = cv2.arcLength(c, True)\n","    approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n","    # if our approximated contour has four points, then we\n","    # can assume that we have found our screen\n","    if len(approx) == 4:\n","      screenCnt = approx \n","      return {'edge_mask':edged,'contour':screenCnt}\n","    \n","  return False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SoKI7Wd5HhD4"},"source":["###**_Autotune edge detection parameters_**\n","\n","Loop over a set of hyperparameter values for the Canny algorithm, determined by the inputs `sigma_init`,`sigma_min`, `step`. Calls `edge_detection` for each value of the set. Stops when a candidate contour has been found and returns the result."]},{"cell_type":"code","metadata":{"id":"S10Dso_DHhD4"},"source":["def autotune_edge_detection(image,sigma_init=0.5,sigma_min=0,step=0.05):\n","   \n","  # create arrays of parameter to scan\n","  sigmas = [sigma_init]\n","  for s in np.arange(sigma_min,sigma_init,step):\n","    sigmas.append(sigma_init+s)\n","    sigmas.append(sigma_init-s)\n","\n","  # loop over parameter values\n","  # if edge detection find a good contour\n","  # return the result\n","  for s in sigmas:\n","    result = edge_detection(image,s)\n","    if result: return result\n","\n","  print('I didnt find the contours I am sorry')\n","  return\n","\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KAbK5ZkL0jMN"},"source":["##**_Read Image_**"]},{"cell_type":"code","metadata":{"id":"QW1DPwj5BcCL"},"source":["#read the original image from disk\n","filename = os.path.join(os.getcwd(),'invoice_ticket.jpg')\n","template = cv2.imread(filename)\n","\n","# Bonus\n","# upload an image to transform\n","#uploaded = files.upload()\n","#uploaded_images = [decode_image(vals) for keys,vals in uploaded.items()]\n","\n","filename = os.path.join(os.getcwd(),'invoice_test_flash.jpg')\n","uploaded_image = cv2.imread(filename)\n","\n","# store a copy of the original image\n","orig = uploaded_image.copy()\n","\n","#show the input image\n","draw(cv2.cvtColor(uploaded_image, cv2.COLOR_BGR2RGB))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l8bEY6_LqA6d"},"source":["As you can see, the image in far from being usable. We need to isolate only the part containing the invoice and try to eliminate the table and the part of my trouser üòÖ"]},{"cell_type":"markdown","metadata":{"id":"a4cpifbK0lpn"},"source":["##**_Perform Edge Detection_**"]},{"cell_type":"markdown","metadata":{"id":"fL0fyNsFp_XS"},"source":["The logical steps to accomplish this mission are the following:\n","\n","\n","1.   find all the possible closed shapes in the image\n","2.   find the shape that has the biggest area and is a rectangle with 4 edges\n","\n","If we find it, we can safely assume that the shape corresponds to the one of the paper invoice.\n","\n","The goal is accomplished by the following technique:\n","\n","*  We apply the [Canny algorithm](https://docs.opencv.org/3.4/da/d5c/tutorial_canny_detector.html) to the image and obtain a mask (a black/withe image) of all the edges.\n","*   We use the [OpenCV find Contours](https://docs.opencv.org/master/d3/dc0/group__imgproc__shape.html#gadf1ad6a0b82947fa1fe3c3d497f260e0) function on the mask to obtain all the closed shapes in the image.\n","* If we don't find anything we repeat the procedure changing the Canny algorithm parameters, until we find a good candidate\n","\n","For a matter of time, we will not go in deep detail of all the inner mechanisms of all the algorithms involved. The functions we will use are defined in the beginning of the notebook under the section **_Utilites functions_**. You can feel free to investigate more about them if you are interested.\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"XRzV1hbFqA4V"},"source":["result = autotune_edge_detection(uploaded_image,sigma_init=0.5)\n","if result:\n","\n","  # draw edge mask\n","  edged = result['edge_mask']\n","  draw(cv2.cvtColor(edged, cv2.COLOR_BGR2RGB))\n","  #cv2.imwrite(os.path.join(os.getcwd(),'edged.jpg'),edged)\n","\n","  # draw contour on the image\n","  cv2.drawContours(uploaded_image, [result['contour']] , -1, (0, 255, 0), 3)\n","  draw(cv2.cvtColor(uploaded_image, cv2.COLOR_BGR2RGB))\n","  #cv2.imwrite(os.path.join(os.getcwd(),'countour.jpg'),uploaded_image)\n","\n","else:\n","  print('No contour was found')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jz6U9J2905rz"},"source":["##**_Warp the Image_**\n","\n","We then use the points corresponding to the rectangle we have just found to create a linear trasformation matrix that will map the points of the rectangle to the points of the template invoice. \n","\n","This steps are exploited using the function ```four_point_transform``` in the section **_Utilities functions_** and the OpenCV functions: \n","\n","```\n","cv2.getPerspectiveTransform()\n","cv2.warpPerspective()\n","  \n"," ```\n","\n"]},{"cell_type":"code","metadata":{"id":"klpfqUSkrbTs"},"source":["# apply the four point transform to obtain a top-down\n","# view of the original image using the copy and the edged mask\n","warped = four_point_transform(orig,template,result['contour'].reshape(4,2))\n","\n","# draw warped image\n","draw(cv2.cvtColor(warped, cv2.COLOR_BGR2RGB))\n","cv2.imwrite(os.path.join(os.getcwd(),'warped.jpg'),warped)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0CYTiSzu9hU8"},"source":["and bam! ü§Ø \n","\n","We have our image taken from camera aligned. From here on we can again apply our well known technique of text extraction from RoIs, and see what is the output result."]},{"cell_type":"markdown","metadata":{"id":"yBcuHE3H_3jK"},"source":["##**_Use Tesseract on Selected RoIs_**"]},{"cell_type":"code","metadata":{"id":"0sDLgYTbxycF"},"source":["def extract_text_from_roi(warped,key='first_name'):\n","  # define a dictionary with roi coordinates\n","  # in the following order\n","  # 1. (top_left_x,top_left_y) \n","  # 2. (bottom_right_x,bottom_right_y)\n","\n","  # determine a scaling factor between the original template\n","  # and the warped image\n","  h_orig, w_orig = 2399,1653\n","  if len(warped.shape) > 2:\n","    h_warped, w_warped, c_warped = warped.shape \n","  else:\n","    h_warped, w_warped = warped.shape \n","\n","  fh = h_warped/h_orig\n","  fw = w_warped/w_orig  \n","\n","  # define a dictionary with the position of the rois\n","  # in the template image\n","  rois = {'first_name':[(1,485),(1652,655)],\n","          'email':     [(1,1240),(1652,1419)],\n","          'to':        [(130,440),(775,600)]} #(1,328),(793,589)]\n","\n","  # scale the roi to match\n","  # the size of the warped image\n","  tlx = rois[key][0][0]*fw\n","  tly = rois[key][0][1]*fh\n","  brx = rois[key][1][0]*fw\n","  bry = rois[key][1][1]*fh\n","\n","  # select the corresponding rois in the image\n","  #aligned = cv2.resize(warped, (w_orig,h_orig), interpolation = cv2.INTER_AREA)\n","  aligned = warped.copy()\n","  roi = aligned[int(tly):int(bry),int(tlx):int(brx)]\n","\n","  # draw the roi and extract the text with tesseract\n","  rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n","  draw(rgb,(25,7))\n","  text = pytesseract.image_to_string(rgb)\n","  for line in text.split(\"\\n\"):\n","      print(line)\n","  \n","  return text\n","\n","text=extract_text_from_roi(warped,key='to')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4j5RmToUAOpG"},"source":["We are still able to extract the information from the RoI but, as you can see, the quality is lowering."]},{"cell_type":"markdown","metadata":{"id":"W9oVJCV1vZII"},"source":["<img src=\"https://drive.google.com/uc?id=1j615H3PrNo15Jz2Zu0H4jn-N6iu5_EIg\" width=\"25%\">\n","\n","#**Case 4 - _Difficulty: Sith Lord_**\n","#_Document with items in variable position: Detect RoIs using Bounding Box Regression with Deep Learning_\n","\n","In many situations the position of the regions we are interested to inspect in a document can vary. An example is in the invoices we've been analyzing: the height of table with the products purchased will vary each time, since the number of products is not always the same.\n","\n","In this case we will not be able at all to extract a RoI and we need some more sophisticated technique. Fortunately we can instruct our machine to detect the position of the RoI, even if it changes every time.\n","\n","How? with Deep learning of course!"]},{"cell_type":"markdown","metadata":{"id":"Lqo3UUbHGPFh"},"source":["##**_Import Keras and TensorFlow_**\n","\n","We will be using Keras and Tensor Flow library for the task, so let's import all the relevant stuff"]},{"cell_type":"code","metadata":{"id":"xH3KkAkMGCEu"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dropout\n","\n","from tensorflow.keras.models import Model\n","\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.optimizers import Adam\n","\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.preprocessing.image import load_img\n","\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qYy8o5CLGnRF"},"source":["##**_Read the Annotations_**\n","\n","To train a model we will need to present it the images of the invoice, and tell it the position of the RoI in each image. We are going to use a json file that has this information and that I have constructed before, using the [VGG Image Annotator](https://www.robots.ox.ac.uk/~vgg/software/via/): a fantastic open source tool for dataset annotation.\n","\n","The file has all the information we need: file name, position of the RoI (expressed in x,y,height and width)."]},{"cell_type":"code","metadata":{"id":"qXgNWGS1VDwj"},"source":["# download the annotation file\n","!curl -L \"https://docs.google.com/uc?export=download&id=1491L9DVNPMvZdaQwVej81BO_RRGw4MIy\" > annotations.json\n","\n","# load the contents of the json annotations file\n","print(\"\\n[INFO] reading json annotations...\\n\")\n","annotations_json = os.path.join(os.getcwd(),'annotations.json')\n","with open(annotations_json) as f:\n","  annotations_dict = json.load(f)\n","\n","#print some elements of the json\n","for i in list(annotations_dict.items())[:3]:\n","  print(i)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y_hz1Xu2pmic"},"source":["If you look at the 'region_label' attribute you can see that the annotations correspond to the bottom right part of the invoice, were the total amount due is registered. The position of this region in fact varies in each invoice."]},{"cell_type":"markdown","metadata":{"id":"p_TMrMVRWO0n"},"source":["##**_Construct the training data_**\n","\n","We construct the training data by looping over all the filenames and creating 2 arrays:\n","\n","1.   with all the images\n","2.   with the coordinates of the Bounding Box\n","\n"]},{"cell_type":"code","metadata":{"id":"7Df5uM4IZ8tH"},"source":["# mount drive folder and import custom modules\n","# drive.mount('/content/drive', force_remount=False)\n","# sys.path.insert(0,'/content/drive/MyDrive/Samurai_Workshop')\n","\n","#dataset_path = os.path.join(sys.path[0],'dataset_jpg','invoice')\n","#output_path= os.path.join(sys.path[0],\"output\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2iCTUb7QWrWp"},"source":["# build path to dataset\n","dataset_path = os.path.join(os.getcwd(),'AMLD2021','dataset','invoice')\n","\n","# initialize the lists\n","data,targets,filenames = [],[],[]\n","\n","print(\"\\n[INFO] reading images... this may take a while ...\\n\")\n","\n","# loop over the keys and values of the json dictionary\n","for key,val in annotations_dict.items():\n","\n","  # do not read if there is no region registered\n","  if len(val['regions']) == 0: continue\n","\n","  # read the relevant info from the annotations dictionary\n","  # filename and x,y coordinates of the bounding box\n","  filename = val['filename']\n","  startX = val['regions'][0]['shape_attributes']['x']\n","  startY = val['regions'][0]['shape_attributes']['y']\n","  endX = startX + val['regions'][0]['shape_attributes']['width'] #startx + width\n","  endY = startY + val['regions'][0]['shape_attributes']['height'] #starty + height\n","\n","  # build the path to the image and read it\n","  image_path = os.path.join(dataset_path,filename)\n","  image = cv2.imread(image_path)\n","\n","  # skip if there was a problem loading the image\n","  if image is None: continue\n","\n","  # scale the bounding box coordinates relative to the spatial\n","  # dimensions of the input image\n","  (h, w) = image.shape[:2]\n","  startX = float(startX) / w\n","  startY = float(startY) / h\n","  endX = float(endX) / w\n","  endY = float(endY) / h\n","  \n","  # load the image and preprocess it\n","  # scale to 224 x 224 the input size for VGG16\n","  image = load_img(image_path, target_size=(224, 224))\n","  image = img_to_array(image)\n","\t\n","  # update our list of data, targets, and filenames\n","  data.append(image)\n","  targets.append((startX, startY, endX, endY))\n","  filenames.append(filename)\n","\n","print(\"\\n[INFO] Read {} total number of images ...\\n\".format(len(data)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gqs_p_mLECnW"},"source":["### **_Train/Test Split_**\n","\n","We normalize the images (scale all pixels in the range [0,1]) and use Scikit-Learn  ```train_test_split()``` function and use 10% of the dataset for testing.\n","\n"]},{"cell_type":"code","metadata":{"id":"foIklGOKEDFg"},"source":["# convert the data and targets to NumPy arrays, scaling the input\n","# pixel intensities from the range [0, 255] to [0, 1]\n","data = np.array(data, dtype=\"float32\") / 255.0\n","targets = np.array(targets, dtype=\"float32\")\n","\n","# partition the data into training and testing splits using 90% of\n","# the data for training and the remaining 10% for testing\n","split = train_test_split(data, targets, filenames, test_size=0.10,random_state=42)\n","\n","# unpack the data split\n","(trainImages, testImages) = split[:2]\n","(trainTargets, testTargets) = split[2:4]\n","(trainFilenames, testFilenames) = split[4:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cT-I39MJE0mA"},"source":["##**_Define Model and Compile_**\n","\n","We are going to use a transfer learning technique to train our model. This means that we will download from Keras a neural network (the [VGG16](https://neurohive.io/en/popular-networks/vgg16/)) that has altready been trained on a huge image dataset ([Imagenet](https://en.wikipedia.org/wiki/ImageNet) in this case).\n","\n","In order to keep the feature extraction power of the inner layers, we will keep the convulutional layers, but we will change only the otuput Fully Connected layers. The Network will have a 4-neuron output layer, since the have to predict 4 numbers.\n","\n","We the handy Keras functional API to do the magic. üßô‚Äç‚ôÇÔ∏è"]},{"cell_type":"code","metadata":{"id":"Yy1zBfodmibw"},"source":["# initialize our initial learning rate and # of epochs to train for\n","INIT_LR = 1e-5\n","EPOCHS = 30\n","BS = 32\n","\n","# load the VGG16 network, ensuring the head FC layer sets are left\n","# off\n","baseModel = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n","\n","# freeze all VGG layers\n","baseModel.trainable = False\n","\n","# flatten the max-pooling output of VGG\n","flatten = baseModel.output\n","flatten = Flatten()(flatten)\n","\n","# construct a fully-connected layer header to output the predicted\n","# bounding box coordinates\n","bboxHead = Dense(128, activation=\"relu\")(flatten)\n","bboxHead = Dense(64, activation=\"relu\")(bboxHead)\n","bboxHead = Dense(32, activation=\"relu\")(bboxHead)\n","bboxHead = Dense(4, activation=\"sigmoid\")(bboxHead)\n","\n","# place the head FC model on top of the base model (this will become\n","# the actual model we will train)\n","model = Model(inputs=baseModel.input, outputs=bboxHead)\n","\n","# compiling the model\n","# we use the mean squared error \n","# a very common loss for regression problems\n","print(\"[INFO] compiling model...\")\n","opt = Adam(learning_rate=INIT_LR)\n","model.compile(loss=\"mse\", optimizer=opt)\n","print(model.summary())\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GFEDrlXfFZhd"},"source":["##**_Train Model_**"]},{"cell_type":"code","metadata":{"id":"S-aJgCaHFdWj"},"source":["# train the network for bounding box regression\n","print(\"\\n[INFO] training bounding box regressor...\\n\")\n","H = model.fit(\n","    trainImages, trainTargets,\n","    validation_data=(testImages, testTargets),\n","    batch_size=BS,\n","    epochs=EPOCHS,\n","    verbose=1)\n","\n","# plot the model training history\n","print(\"\\n[INFO] Drawing the Training Curves...\\n\\n\")\n","N = EPOCHS\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n","plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n","plt.title(\"Bounding Box Regression Loss on Training Set\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Loss\")\n","plt.legend(loc=\"lower left\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S1uGK5EbopSy"},"source":["A nice and smooth training Curve after very few Epochs. No sign of overfitting. \n","\n","We are know ready to use this model and make some predictions to see if we can extract the RoI we want from some invoices."]},{"cell_type":"markdown","metadata":{"id":"UmqNGx4MER8d"},"source":["##**_Make predictions_**\n","\n","Always remember. The input image for the prediction should have the exact same size and format that the ones used for training. This is why it is extremely important to reapeat the pre-processing steps before prediciting our RoI position."]},{"cell_type":"code","metadata":{"id":"Rn1cGJIT5Mmk"},"source":["# dowload the test image with curl and subprocess\n","#\n","# list of available images:\n","# \n","# https://docs.google.com/uc?export=download&id=1YufQe63RzA04Up9mHhGNf2VpRhztcZoG\n","# https://docs.google.com/uc?export=download&id=1oUy07v1t-R5h0tAJKGpxzhQ7ALrCHlmx\n","# https://docs.google.com/uc?export=download&id=1xtNq_HB0u8-oD9NeOjLrs6zlRVOG9ZFp\n","# https://docs.google.com/uc?export=download&id=1YufQe63RzA04Up9mHhGNf2VpRhztcZoG\n","\n","urls = ['https://docs.google.com/uc?export=download&id=1YufQe63RzA04Up9mHhGNf2VpRhztcZoG',\n","        'https://docs.google.com/uc?export=download&id=1oUy07v1t-R5h0tAJKGpxzhQ7ALrCHlmx',\n","        'https://docs.google.com/uc?export=download&id=1xtNq_HB0u8-oD9NeOjLrs6zlRVOG9ZFp',\n","        'https://docs.google.com/uc?export=download&id=1YufQe63RzA04Up9mHhGNf2VpRhztcZoG']\n","\n","url = urls[1]\n","outname = os.path.join(os.getcwd(),'test.jpg')\n","subprocess.run(['curl','-L',url,'-o',outname])\n","\n","# load the input image (in Keras format) from disk and preprocess\n","# it, scaling the pixel intensities to the range [0, 1]\n","# adding the batch dimension\n","image = load_img(outname, target_size=(224, 224))\n","image = img_to_array(image) / 255.0\n","image = np.expand_dims(image, axis=0)\n","\n","# make bounding box predictions on the input image\n","preds = model.predict(image)[0]\n","(startX, startY, endX, endY) = preds\n","\t\n","# load the input image and grab its dimensions\n","test = cv2.imread(outname)\n","(h, w) = test.shape[:2]\n","\t\n","# scale the predicted bounding box coordinates based on the image\n","# dimensions\n","startX = int(startX * w)\n","startY = int(startY * h)\n","endX = int(endX * w)\n","endY = int(endY * h)\n","\t\n","#  draw the predicted bounding box on the image\n","cv2.rectangle(test, (startX, startY), (endX, endY),(0, 255, 0), 2)\n","plt.figure(figsize=(7,25))\n","plt.imshow(cv2.cvtColor(test, cv2.COLOR_BGR2RGB))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oQJbSI7qrlb4"},"source":["As you can see, the model has correctly spot the region of the invoice we were looking for. From here we can apply again the methodology of extracting text from RoIs we discussed before, but for the moment ... one last battle before leaving!"]},{"cell_type":"markdown","metadata":{"id":"WTWBrwIe5qt2"},"source":["#**_Exercise_**\n","##_**The Last battle: train your own model**_ \n","\n","---\n","\n","<br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1SbIBJfPHw5mIDL7T0QwtN6iQchm_Lm6G\" width=\"50%\">\n","\n","---\n","\n","<br>\n","\n","Please follow the instructions:\n","\n","#### **1.** _Dowload the VGG Image Annotator (VIA) zip file from the [tools folder](https://drive.google.com/file/d/1Wm1pOwJJgfWY78gvwsiDFHwf-r14FSi3/view?usp=sharing) or from the [official site](https://www.robots.ox.ac.uk/~vgg/software/via/downloads/via-2.0.11.zip), unzip it and open the via.html file with your browser_.\n","\n","#### **2.** _Dowload the dataset from [here](https://drive.google.com/drive/folders/10xe9yFCkRSPcRfJe3l0M57ltYpSZZg4z?usp=sharing)_.\n","\n","#### **3.** _Use VIA to_: \n","  * Upload the images from _300 on\n","  * Note the position of the Bounding Box relative to the table of products of the images from _300 on (You can do as much as you want until the end of the dataset _435. I suggest you to do max 50 images for timing reasons)\n","  * Export the annotations in .json format.\n","\n","  * (TIP: read the 'help' section in the VIA software)\n","\n","#### **4.** _Run the cell below and upload the .json annotations you just made: the cell will merge yours with the first _300 that I took last night instead of watching a movie._ üòù\n"]},{"cell_type":"code","metadata":{"id":"kYsYqlMkbPFm"},"source":["# download annotation json file\n","url = \"https://docs.google.com/uc?export=download&id=1tfGeWrTiFgPsq21ULWcvavEwlBue-6E9\"\n","outname = os.path.join(os.getcwd(),'annotations_invoice_table_0_299.json')\n","subprocess.run(['curl','-L',url,'-o',outname])\n","\n","# read first downloaded annotations json from disk \n","# as dictionary \n","with open(outname) as f:\n","  annotations_dict = json.load(f)\n","\n","# upload the json annotation from remote\n","# and read it as a dictionary\n","uploaded = files.upload()\n","data = next(iter(uploaded.values()))\n","annotations_dict1 = json.loads(data.decode())\n","\n","# set items in first dictionary equal\n","# to the elements in the uploaded dictionary\n","for key,val in annotations_dict1.items():\n","  if len(val['regions']) > 0:\n","    annotations_dict[key] = val\n","\n","# clean the merged dictionary\n","# eliminate the elements with no region\n","keys = [key for key,val in annotations_dict.items() if len(val['regions'])==0]\n","for k in keys: del annotations_dict[k]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_rTgsO45UC-s"},"source":["#### **5.** _Try to train a bounding box regression model to find the table position using transfer learning._\n","#### **6.** _Make some predictions on the test images used before._\n","#### **7.** _Try to extract some information using Tesseract._\n","<br>\n","\n","### **N.B.** copy+paste the cells from **Construct the training data** to **Make prediction** will surely do the job!"]},{"cell_type":"code","metadata":{"id":"THezkcgXqMsv"},"source":["### Start your exercise from here!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z88eX396tqOK"},"source":["#**_Congratulations!_**\n","\n","<img src=\"https://drive.google.com/uc?id=1n8MRZfFFlS8NNjSEV1aysp2__qabAzbw\" width=\"50%\">\n","\n","#**_You arrived at the end of the Module_**"]}]}